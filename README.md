# FCSE-Skopje 2023 Undergraduate Study Programs Preprocessor

The preprocessor application transforms, cleans, and validates raw study program data sourced from the 
**[Faculty of Computer Science and Engineering (FCSE)](https://finki.ukim.mk)** at the 
**[Ss. Cyril and Methodius University in Skopje](https://www.ukim.edu.mk)**, preparing it for analytical use.

It consumes raw data, uses **Pandas** for data transformation, and outputs a set of normalized tables into a **PyIceberg** lakehouse data storage.

---

## Requirements

- **Data Source:** This application requires the raw data output generated by the
    **[undergraduate-study-program-scraper](https://github.com/username-gigo-is-not-available/undergraduate-study-programs-scraper)**.
- **Python 3.9 or later**.
- Access to an **Iceberg Catalog** and **MinIO server** (for S3 storage) or local disk space (for local storage).

---

## Overview: Data Pipeline

The preprocessor executes a multistep pipeline for five main entities, transforming raw data from the `raw` namespace
into fully structured and linked datasets in the `processed` namespace.

### Pipeline Stages

| Entity            | Core Operations                                                                                                                                       | Output Datasets                       |
|:------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------|
| **Study Program** | Load, Clean, **Extract** Code, **Generate** ID, Store.                                                                                                | `study_programs`                      |
| **Course**        | Load, Clean, **Extract** Level & Abbreviation, **Generate** ID, Store.                                                                                | `courses`                             |
| **Professor**     | Load, Clean, **Flatten** names, **Extract** Name/Surname, **Generate** ID.                                                                            | `professors`, `teaches`               |
| **Requisites**    | Load, Clean, **Match** course names, **Flatten** prerequisites, **Generate** IDs, Store.                                                              | `requisites`, `requires`, `satisfies` |
| **Curriculum**    | Load, Clean, Merge, **Extract** Semester Season/Year, **Generate** IDs, **Invalidate** (remove courses lacking prerequisites offered by the program). | `curricula`, `offers`, `includes`     |

---

### Data Output

The preprocessor saves the data into an Apache Iceberg lakehouse structure within a specified namespace (ICEBERG_DESTINATION_NAMESPACE). The output is structured into 10 normalized tables:

| Dataset                 | Core Columns                                                                             |                                                                                    
|:------------------------|:-----------------------------------------------------------------------------------------|
| 1. **`study_programs`** | `study_program_id`, `study_program_code`, `study_program_name`, `study_program_duration` |                                                   
| 2. **`courses`**        | `course_id`, `course_code`, `course_name_mk`, `course_level`, `course_abbreviation`      |
| 3. **`professors`**     | `professor_id`, `professor_name`, `professor_surname`                                    |                                                                                    |
| 4. **`curricula`**      | `curriculum_id`, `course_type`, `course_semester`, `course_academic_year`                |
| 5. **`requisites`**     | `requisite_id`, `course_prerequisite_type`, `minimum_required_number_of_courses`         |
| **Relationship Tables** | `offers`, `includes`, `teaches`, `requires`, `satisfies`                                 |

---

## Installation and Setup

1. **Clone the repository**
   ```bash
   git clone <repository_url>
   ```

2. **Install the required packages**
   ```bash
   pip install -r requirements.txt
   ```

3. **Configuration Files**
   Ensure you have the necessary environment configuration. This typically involves:
    * A **`.env`** file for environment variables.
    * A **`.pyiceberg.yaml`** file for Iceberg catalog configuration.

---

## Environment Variables

The application is configured using environment variables, which define storage access and Iceberg table names.

### General Configuration

| Variable         | Description                                                                                               |
|:-----------------|:----------------------------------------------------------------------------------------------------------|
| `FILE_IO_TYPE`   | Storage type for the lakehouse metadata. Must be **`local`** or **`s3`**.                                 |

### Iceberg Configuration (Metastore)

| Variable                        | Description                                                                                               |
|:--------------------------------|:----------------------------------------------------------------------------------------------------------|
| `PYICEBERG_HOME`                | The internal path where the application is executed (e.g., `/undergraduate-study-programs-preprocessor`). |
| `ICEBERG_CATALOG_NAME`          | The name of the catalog configuration (e.g., `default`) used to connect to the metastore.                 |
| `ICEBERG_SOURCE_NAMESPACE`      | The database/schema where **raw input data** is located (e.g., `raw`).                                    |
| `ICEBERG_DESTINATION_NAMESPACE` | The database/schema where **processed output data** will be written (e.g., `processed`).                  |

### Storage-Specific Configuration

#### 1. Local Storage (`FILE_IO_TYPE="LOCAL"`)

| Variable                            | Description                                                                                                      |
|:------------------------------------|:-----------------------------------------------------------------------------------------------------------------|
| `LOCAL_ICEBERG_LAKEHOUSE_FILE_PATH` | The **absolute path** to the root directory that serves as the Iceberg warehouse (e.g., `/app/local_lakehouse`). |

#### 2. MinIO/S3 Storage (`FILE_IO_TYPE="S3"`)

| Variable                           | Description                                                                                                                |
|:-----------------------------------|:---------------------------------------------------------------------------------------------------------------------------|
| `S3_ENDPOINT_URL`                  | The full endpoint URL (host and port) of the MinIO server (e.g., `localhost:9000`).                                        |
| `S3_ACCESS_KEY`                    | The access key ID required for MinIO authentication.                                                                       |
| `S3_SECRET_KEY`                    | The secret access key required for MinIO authentication.                                                                   |
| `S3_ICEBERG_LAKEHOUSE_BUCKET_NAME` | The name of the S3 bucket that will host the Iceberg warehouse (e.g., `finki-warehouse`).                                  |
| `S3_PATH_STYLE_ACCESS`             | Boolean flag (`True`/`False`) indicating whether to use path-style addressing (required for MinIO or custom S3 endpoints). |

---

### Dataset Naming Configuration

These variables allow flexibility in naming the 10 resulting Iceberg tables:

| Variable                      | Description                                                       |
|:------------------------------|:------------------------------------------------------------------|
| `STUDY_PROGRAMS_DATASET_NAME` | Output table name for study programs (e.g., `study_programs`).    |
| `COURSES_DATASET_NAME`        | Output table name for courses (e.g., `courses`).                  |
| `CURRICULA_DATASET_NAME`      | Output table name for curriculum details (e.g., `curricula`).     |
| `REQUISITES_DATASET_NAME`     | Output table name for requisites (e.g., `requisites`).            |
| `PROFESSORS_DATASET_NAME`     | Output table name for professors (e.g., `professors`).            |
| `OFFERS_DATASET_NAME`         | Output table name for offers relationship (e.g., `offers`).       |
| `INCLUDES_DATASET_NAME`       | Output table name for includes relationship (e.g., `includes`).   |
| `REQUIRES_DATASET_NAME`       | Output table name for requires relationship (e.g., `requires`).   |
| `SATISFIES_DATASET_NAME`      | Output table name for satisfies relationship (e.g., `satisfies`). |
| `TEACHES_DATASET_NAME`        | Output table name for teaches relationship (e.g., `teaches`).     |

---

## Running the Preprocessor

### Option 1: Local Execution

Ensure all environment variables are loaded (e.g., `source .env` on Linux/macOS or equivalent in PowerShell).

```bash
python run.py
```
### Option 2: Docker Compose (Recommended for S3/MinIO)

If your setup uses MinIO, use Docker Compose to manage both the Python application and the supporting services.

```bash
docker compose up
```
